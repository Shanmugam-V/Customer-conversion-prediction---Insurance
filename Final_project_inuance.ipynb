import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix, classification_report
import pickle
from flask import Flask, request, jsonify

def load_data(file_path):
    return pd.read_csv(file_path)

file_path = "C:/Users/Shanmugam.V/OneDrive/Desktop/Final_Project-Insurance/data.txt"
data = load_data(file_path)
print(data.head())

# Replace 'unknown' with NaN
data.replace('unknown', np.nan, inplace=True)

# Handle missing values
for col in data.columns:
    if data[col].dtype == 'object':  # Categorical columns
        data[col].fillna(data[col].mode()[0], inplace=True)
    else:  # Numerical columns
        data[col].fillna(data[col].median(), inplace=True)

# Map months to numeric values
month_mapping = {'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6,
                 'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12}
data['mon'] = data['mon'].map(month_mapping)

# One-hot encode categorical columns
categorical_cols = ['job', 'marital', 'education_qual', 'call_type', 'prev_outcome']
data = pd.get_dummies(data, columns=categorical_cols, drop_first=True)

# Label encode the target variable
le = LabelEncoder()
data['Outcome'] = le.fit_transform(data['Outcome'])

# Scale numerical columns
scaler = StandardScaler()
numerical_cols = ['age', 'dur', 'num_calls']
data[numerical_cols] = scaler.fit_transform(data[numerical_cols])

# Save preprocessed data
data.to_csv('preprocessed_data.csv', index=False)
print("Data preprocessing complete!")

# Load preprocessed data
data = pd.read_csv('preprocessed_data.csv')

# Univariate Analysis
plt.figure(figsize=(8, 5))
sns.countplot(x='Outcome', data=data)
plt.title('Distribution of Outcome')
plt.show()

# Pairplot of numerical features and Outcome
sns.pairplot(data, vars=['age', 'dur', 'num_calls'], hue='Outcome')
plt.show()

# Duration vs Outcome
plt.figure(figsize=(8, 6))
sns.boxplot(data=data, x='Outcome', y='dur')
plt.title('Duration vs Outcome')
plt.show()


# Split data into features (X) and target (y)
X = data.drop(columns=['Outcome'])
y = data['Outcome']

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

print(f"Training set size: {X_train.shape}")
print(f"Test set size: {X_test.shape}")

# Initialize the model
model = LogisticRegression()

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Initialize the model
rf_model = RandomForestClassifier(random_state=42)

# Train the model
rf_model.fit(X_train, y_train)

# Make predictions
y_pred_rf = rf_model.predict(X_test)

# Evaluate the model
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_rf))
print("\nClassification Report:\n", classification_report(y_test, y_pred_rf))

# Feature Importance (Optional: Analyze the importance of each feature)
feature_importances = pd.Series(rf_model.feature_importances_, index=X.columns)
feature_importances.sort_values(ascending=False, inplace=True)

plt.figure(figsize=(10, 6))
feature_importances.plot(kind='bar')
plt.title('Feature Importance in Random Forest')
plt.ylabel('Importance')
plt.xlabel('Feature')
plt.show()

# Evaluate model performance
def evaluate_model(y_test, y_pred, y_pred_proba=None, model_name="Model"):
    print(f"\n=== {model_name} Evaluation ===")
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("Precision:", precision_score(y_test, y_pred))
    print("Recall:", recall_score(y_test, y_pred))
    print("F1-Score:", f1_score(y_test, y_pred))
    
    if y_pred_proba is not None:  # If probabilities are available
        auc = roc_auc_score(y_test, y_pred_proba[:, 1])
        print("ROC-AUC Score:", auc)
        # Plot ROC Curve
        fpr, tpr, _ = roc_curve(y_test, y_pred_proba[:, 1])
        plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, color='blue', label=f'{model_name} (AUC = {auc:.2f})')
        plt.plot([0, 1], [0, 1], color='red', linestyle='--')
        plt.title(f"ROC Curve - {model_name}")
        plt.xlabel("False Positive Rate")
        plt.ylabel("True Positive Rate")
        plt.legend()
        plt.grid()
        plt.show()

# For Random Forest
y_pred_rf = rf_model.predict(X_test)
y_pred_proba_rf = rf_model.predict_proba(X_test)  # Predicted probabilities for ROC-AUC
evaluate_model(y_test, y_pred_rf, y_pred_proba_rf, model_name="Random Forest")

# Get feature importance from the Random Forest model
feature_importances = pd.Series(rf_model.feature_importances_, index=X.columns)
feature_importances.sort_values(ascending=False, inplace=True)

# Display top features
print("\n=== Feature Importance ===")
print(feature_importances.head(10))  # Top 10 important features

# Visualize feature importance
plt.figure(figsize=(12, 6))
feature_importances.plot(kind='bar', color='skyblue')
plt.title('Feature Importance in Random Forest')
plt.xlabel('Features')
plt.ylabel('Importance')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Create a dictionary to store results
results = {
    "Model": [],
    "Accuracy": [],
    "Precision": [],
    "Recall": [],
    "F1-Score": [],
    "ROC-AUC": []
}

# Add Random Forest evaluation
results["Model"].append("Random Forest")
results["Accuracy"].append(accuracy_score(y_test, y_pred_rf))
results["Precision"].append(precision_score(y_test, y_pred_rf))
results["Recall"].append(recall_score(y_test, y_pred_rf))
results["F1-Score"].append(f1_score(y_test, y_pred_rf))
results["ROC-AUC"].append(roc_auc_score(y_test, y_pred_proba_rf[:, 1]))

# Convert results to DataFrame
results_df = pd.DataFrame(results)

# Display results
print("\n=== Model Comparison ===")
print(results_df)

# Save the trained Random Forest model
with open('random_forest_model.pkl', 'wb') as f:
    pickle.dump(rf_model, f)

print("Model saved successfully!")

# Load the saved model
with open('random_forest_model.pkl', 'rb') as f:
    loaded_model = pickle.load(f)

print("Model loaded successfully!")
